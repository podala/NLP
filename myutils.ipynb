{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "CURRENT_FOLDER = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_FOLDER = os.path.join(CURRENT_FOLDER, 'data')\n",
    "\n",
    "\n",
    "class DataFrameFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Features extraction transformer for pandas dataframe.\n",
    "    DataFrameFeatures transform selected columns of pandas dataframe into numpy\n",
    "    array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_cols : list, default=[]\n",
    "        List of numerical columns to extract\n",
    "    cat_cols : list, default=[]\n",
    "        List of categorical columns to extract\n",
    "    one_hot_drop : bool, default=True\n",
    "        Specify whether to drop a category in each categorical feature. If true,\n",
    "        it drops the last feature after features being sorted.\n",
    "    Attributes\n",
    "    ----------\n",
    "    feature_names_: A list of names corresponding columns of tranformed array.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_cols=[],\n",
    "                 cat_cols=[],\n",
    "                 one_hot_drop=True):\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.one_hot_drop = one_hot_drop\n",
    "\n",
    "    def check_features_existed(self, X):\n",
    "        if type(X) is not pd.DataFrame:\n",
    "            raise TypeError(f'Expect {pd.DataFrame}, but get type {type(X)}')\n",
    "\n",
    "        cols = set(X.columns)\n",
    "        for c in self.num_cols + self.cat_cols:\n",
    "            if c not in cols:\n",
    "                raise ValueError(f'X does not have column `{c}`')\n",
    "\n",
    "    def check_categorial_labels(self, col_name, all_labels, in_labels):\n",
    "        for label in in_labels:\n",
    "            if label not in all_labels:\n",
    "                raise ValueError(\n",
    "                    f'Unseen label `{label}` for column `{col_name}`')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.check_features_existed(X)\n",
    "        self.feature_names_ = []\n",
    "        self.feature_names_.extend(self.num_cols)\n",
    "        self._cat_mapping = {}\n",
    "        for cat in self.cat_cols:\n",
    "            all_labels = sorted(X[cat].unique())\n",
    "            labels = all_labels\n",
    "            if self.one_hot_drop:\n",
    "                labels = all_labels[:-1]\n",
    "            self.feature_names_.extend(f'{cat}_{c}' for c in labels)\n",
    "            self._cat_mapping[cat] = (all_labels, labels)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self, '_cat_mapping')\n",
    "        self.check_features_existed(X)\n",
    "\n",
    "        arr = X[self.num_cols].to_numpy()\n",
    "        for cat in self.cat_cols:\n",
    "            all_labels, labels = self._cat_mapping[cat]\n",
    "            self.check_categorial_labels(cat, all_labels, X[cat].unique())\n",
    "            one_hot = np.zeros((X.shape[0], len(labels)), dtype=np.int)\n",
    "            for i, label in enumerate(labels):\n",
    "                one_hot[:, i] = X[cat] == label\n",
    "            arr = np.concatenate([arr, one_hot], axis=1)\n",
    "        return arr\n",
    "\n",
    "\n",
    "_DEFAULT_DATA_FOLDER = DATA_FOLDER\n",
    "\n",
    "\n",
    "def _path_for(filename, folder):\n",
    "    return os.path.join(folder, filename)\n",
    "\n",
    "\n",
    "_TABLE_CACHES = {}\n",
    "\n",
    "\n",
    "def load_tables(folder=_DEFAULT_DATA_FOLDER, use_cache=True):\n",
    "    \n",
    "    \"\"\"Load all CoverMyMeds data into dataframes.\n",
    "    Args:\n",
    "        folder (str, optional): path to the data folder. Defaults to `./data`.\n",
    "        use_cache (bool, optional): Whether or not to use cache. Defaults to True.\n",
    "            This will make subsequent calls of this function faster.\n",
    "    Returns:\n",
    "        dict: A dict of all tables. Here are the available tables:\n",
    "            dim_date -> dataframe from dim_date.csv\n",
    "            dim_claims -> dataframe from dim_claims.csv\n",
    "            dim_pa -> dataframe from dim_pa.csv\n",
    "            bridge -> dataframe from bridge.csv\n",
    "            full -> join of all above dataframes\n",
    "            dim_pa_full -> similar to dim_pa, but with all possible columns from full\n",
    "            no_pa -> dataframe of approved pharmacy claims\n",
    "    \"\"\"\n",
    "\n",
    "    if use_cache and folder in _TABLE_CACHES:\n",
    "        return _TABLE_CACHES[folder]\n",
    "\n",
    "    df_date = pd.read_csv(_path_for('dim_date.csv', folder))\n",
    "    df_claims = pd.read_csv(_path_for('dim_claims.csv', folder))\n",
    "    df_pa = pd.read_csv(_path_for('dim_pa.csv', folder))\n",
    "    df_bridge = pd.read_csv(_path_for('bridge.csv', folder))\n",
    "    \n",
    "    df_claims['reject_code'] = df_claims.reject_code.fillna(0).astype(int)\n",
    "    df_claims['bin'] = df_claims.bin.astype(str)\n",
    "\n",
    "    df_full = pd.merge(df_claims, df_bridge, on='dim_claim_id')\n",
    "    df_full = pd.merge(df_full, df_pa, how='left', on='dim_pa_id')\n",
    "    df_full = pd.merge(df_full, df_date, how='left', on='dim_date_id')\n",
    "\n",
    "    df_with_pa = df_full[~np.isnan(df_full.pa_approved)].copy()\n",
    "    pa_cols = ['correct_diagnosis', 'tried_and_failed',\n",
    "               'contraindication', 'pa_approved']\n",
    "    for c in pa_cols:\n",
    "        df_with_pa[c] = df_with_pa[c].astype(int)\n",
    "\n",
    "    df_without_pa = df_full[np.isnan(df_full.pa_approved)].copy()\n",
    "    df_without_pa = df_without_pa.drop(pa_cols, axis=1)\n",
    "\n",
    "    tables = {\n",
    "        'dim_date': df_date,\n",
    "        'dim_claims': df_claims,\n",
    "        'dim_pa': df_pa,\n",
    "        'bridge': df_bridge,\n",
    "        'full': df_full,\n",
    "        'dim_pa_full': df_with_pa,\n",
    "        'no_pa': df_without_pa\n",
    "    }\n",
    "\n",
    "    _TABLE_CACHES[folder] = tables\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
